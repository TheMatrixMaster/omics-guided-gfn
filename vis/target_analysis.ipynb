{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hydra\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from pytorch_lightning import (\n",
    "    LightningDataModule,\n",
    "    seed_everything,\n",
    ")\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multimodal_contrastive.analysis.utils import *\n",
    "from multimodal_contrastive.utils import utils\n",
    "\n",
    "# register custom resolvers if not already registered\n",
    "OmegaConf.register_new_resolver(\"sum\", lambda input_list: np.sum(input_list), replace=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How difficult is it to find molecules that produce similar morphological profile to the target?\n",
    "\n",
    "We investigate by plotting a histogram of cosine distances between the joint target/morph latent and the struct latent of all molecules in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shuffled_scaffold'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_name = \"puma_sm_gmc\"\n",
    "configs_path = \"../multimodal_contrastive/configs\"\n",
    "\n",
    "with hydra.initialize(version_base=None, config_path=configs_path):\n",
    "    cfg = hydra.compose(config_name=config_name)\n",
    "\n",
    "cfg.datamodule.split_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for random number generators in pytorch, numpy and python.random\n",
    "# and especially for generating the same data splits for the test set\n",
    "if cfg.get(\"seed\"):\n",
    "    seed_everything(cfg.seed, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on samples from shuffled_scaffold.\n",
      "Train on 13582 samples.\n",
      "Validate on 1698 samples.\n",
      "Test on 1698 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load test data split\n",
    "datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule)\n",
    "datamodule.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/s/stephen.lu/miniconda3/envs/gfn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "100%|██████████| 133/133 [00:46<00:00,  2.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get the raw morphology features\n",
    "test_loader = datamodule.infer_dataloader()\n",
    "loader = make_eval_data_loader(datamodule.dataset)\n",
    "train_idx, val_idx, test_idx = datamodule.get_split_idx()\n",
    "mods = unroll_dataloader(loader, mods=['morph'])\n",
    "morph = mods['morph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Instantiating torch.nn.module JointEncoder\n",
      "INFO:root:Instantiating lightning model <multimodal_contrastive.networks.models.GMC_PL>\n",
      "/home/mila/s/stephen.lu/miniconda3/envs/gfn/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'encoder_joint' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['encoder_joint'])`.\n",
      "  rank_zero_warn(\n",
      "/home/mila/s/stephen.lu/miniconda3/envs/gfn/lib/python3.10/site-packages/pytorch_lightning/utilities/parsing.py:196: UserWarning: Attribute 'common_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['common_encoder'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "# ckpt_path = \"/home/mila/s/stephen.lu/gfn_gene/res/mmc/morph_struct.ckpt\"\n",
    "# ckpt_path = \"/home/mila/s/stephen.lu/gfn_gene/res/mmc/models/epoch=72-step=7738.ckpt\"\n",
    "# ckpt_path = \"/home/mila/s/stephen.lu/scratch/mmc/omics-guided-gfn/lnbstu57/checkpoints/epoch=19-step=2340.ckpt\"\n",
    "ckpt_path = \"/home/mila/s/stephen.lu/scratch/mmc/omics-guided-gfn/bmw9vp7a/checkpoints/epoch=120-step=14157.ckpt\"\n",
    "# ckpt_path = \"/home/mila/s/stephen.lu/scratch/mmc/omics-guided-gfn/05tjweok/checkpoints/epoch=22-step=2438.ckpt\"\n",
    "model = utils.instantiate_model(cfg)\n",
    "model = model.load_from_checkpoint(ckpt_path, map_location=device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [00:59<00:00,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get latent representations for full dataset\n",
    "representations, mols = model.compute_representation_dataloader(\n",
    "    loader,\n",
    "    device=device,\n",
    "    return_mol=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_representations(representations, mols, \"puma_embeddings.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latent representations for test dataset\n",
    "representations_test, mols_test = model.compute_representation_dataloader(\n",
    "    test_loader,\n",
    "    device=device,\n",
    "    return_mol=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_latents_train = representations['struct'][train_idx]\n",
    "struct_latents_val = representations['struct'][val_idx]\n",
    "struct_latents_test = representations['struct'][test_idx]\n",
    "\n",
    "struct_latents = {\n",
    "    \"train\": struct_latents_train,\n",
    "    \"val\": struct_latents_val,\n",
    "    \"test\": struct_latents_test\n",
    "}\n",
    "\n",
    "target_indices = [   39,   338,   903,  1847,  2288,  4331,  6888,  8206,  8838,  8949,\n",
    "        9277,  9300,  9445,  9476, 10075, 12071, 13905]\n",
    "# target_indices = [3608, 16162]\n",
    "# target_indices = [15907, 16487, 15769, 8899, 7446]\n",
    "n_targets = len(target_indices)\n",
    "# n_targets = 5\n",
    "\n",
    "fig, ax = plt.subplots(n_targets, 3, figsize=(15, 5*n_targets))\n",
    "\n",
    "for i in range(n_targets):\n",
    "    # get a random target molecule and its latent representations\n",
    "    target_idx = target_indices[i]\n",
    "    # target_idx = np.random.choice(test_idx)\n",
    "    # assert target_idx in test_idx\n",
    "\n",
    "    target_joint_latent = representations['joint'][target_idx]\n",
    "    target_morph_latent = representations['morph'][target_idx]\n",
    "    # target_joint_latent = representations_test['joint'][target_idx]\n",
    "    # target_morph_latent = representations_test['morph'][target_idx]\n",
    "\n",
    "    for j, split in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        struct_latents_split = struct_latents[split]\n",
    "\n",
    "        # compute cosine similarity between target and all struct latents\n",
    "        cosine_sim_joint = cosine_similarity([target_joint_latent], struct_latents_split)\n",
    "        cosine_sim_morph = cosine_similarity([target_morph_latent], struct_latents_split)\n",
    "\n",
    "        # plot histogram of cosine sim\n",
    "        ax[i, 0].hist(cosine_sim_joint.flatten(), bins=50, alpha=0.5, label=split)\n",
    "        ax[i, 1].hist(cosine_sim_morph.flatten(), bins=50, alpha=0.5, label=split)\n",
    "\n",
    "    ax[i, 0].set_title(f\"Cosine similarity to joint target {target_idx}\")\n",
    "    ax[i, 1].set_title(f\"Cosine similarity to morph target {target_idx}\")\n",
    "\n",
    "    # plot the target molecule\n",
    "    mol = mols[target_idx]\n",
    "    mol = Chem.MolFromSmiles(mol)\n",
    "    img = Draw.MolToImage(mol)\n",
    "    \n",
    "    num_atoms = mol.GetNumAtoms()\n",
    "    num_bonds = mol.GetNumBonds()\n",
    "\n",
    "    ax[i, 2].set_title(f\"Num atoms: {num_atoms}, Num bonds: {num_bonds}\")\n",
    "    ax[i, 2].imshow(img)\n",
    "    ax[i, 2].axis(\"off\")\n",
    "\n",
    "    # save the target instance\n",
    "    sample = loader.dataset[target_idx]\n",
    "    struct = sample[\"inputs\"][\"struct\"]\n",
    "    smiles = struct.mols\n",
    "\n",
    "    # with open(f\"sample_{target_idx}.pkl\", \"wb\") as f:\n",
    "    #     pickle.dump(sample, f)\n",
    "    \n",
    "    # with open(f\"sample_{target_idx}.txt\", \"wb\") as f:\n",
    "    #     f.write(smiles)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get summary statistics about the molecules in the training dataset\n",
    "I want to get statistics on number of nodes, number of edges, and types of fragments on the molecules in the dataset. This will guide the hyperparameters that we set on the gflownet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"train\": train_idx,\n",
    "    \"val\": val_idx,\n",
    "    \"test\": test_idx\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    num_edges = []\n",
    "    num_nodes = []\n",
    "\n",
    "    for idx in datasets[split]:\n",
    "        mol = mols[idx]\n",
    "        mol = Chem.MolFromSmiles(mol)\n",
    "\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        num_bonds = mol.GetNumBonds()\n",
    "\n",
    "        num_nodes.append(num_atoms)\n",
    "        num_edges.append(num_bonds)\n",
    "\n",
    "    ax[0].hist(num_edges, bins=50, alpha=0.5, label=split)\n",
    "    ax[1].hist(num_nodes, bins=50, alpha=0.5, label=split)\n",
    "\n",
    "ax[0].set_title(\"Number of edges in molecules\")\n",
    "ax[1].set_title(\"Number of nodes in molecules\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's take a look at what the fragments in the gflownet frag building env look like\n",
    "from gflownet.models import bengio2021flow\n",
    "\n",
    "smi, stems = zip(*bengio2021flow.FRAGMENTS)\n",
    "\n",
    "num_atoms = []\n",
    "num_edges = []\n",
    "\n",
    "for smile in smi:\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    num_atoms.append(mol.GetNumAtoms())\n",
    "    num_edges.append(mol.GetNumBonds())\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(num_atoms, bins=25)\n",
    "ax[0].set_title(\"Number of atoms per fragment\")\n",
    "ax[0].set_xlabel(\"Number of atoms\")\n",
    "ax[0].set_ylabel(\"Count (#fragments)\")\n",
    "\n",
    "ax[1].hist(num_edges, bins=25)\n",
    "ax[1].set_title(\"Number of edges per fragment\")\n",
    "ax[1].set_xlabel(\"Number of edges\")\n",
    "ax[1].set_ylabel(\"Count (#fragments)\")\n",
    "\n",
    "# Add vertical bar for mean\n",
    "mean_atoms = np.mean(num_atoms)\n",
    "mean_edges = np.mean(num_edges)\n",
    "\n",
    "ax[0].axvline(mean_atoms, color='r', linestyle='--', label=f\"Mean: {mean_atoms:.2f}\")\n",
    "ax[1].axvline(mean_edges, color='r', linestyle='--', label=f\"Mean: {mean_edges:.2f}\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 [[0.48036456]] [[0.9193298]] [[0.5960577]]\n",
      "338 [[0.08416305]] [[0.88583714]] [[0.35768402]]\n",
      "903 [[0.45698345]] [[0.9567528]] [[0.63815033]]\n",
      "1847 [[0.16040224]] [[0.95529324]] [[0.269818]]\n",
      "2288 [[0.72375035]] [[0.93877786]] [[0.81551504]]\n",
      "4331 [[-0.33291197]] [[0.93131626]] [[-0.09960501]]\n",
      "6888 [[0.22609639]] [[0.90282863]] [[0.4882638]]\n",
      "8206 [[-0.11188747]] [[0.91880214]] [[0.15454614]]\n",
      "8838 [[0.6055174]] [[0.8084939]] [[0.8317271]]\n",
      "8949 [[0.16585147]] [[0.8484664]] [[0.29815197]]\n",
      "9277 [[0.64228374]] [[0.9015377]] [[0.7432953]]\n",
      "9300 [[0.12547535]] [[0.86142755]] [[0.48697382]]\n",
      "9445 [[0.4038251]] [[0.95098704]] [[0.47925556]]\n",
      "9476 [[0.6902788]] [[0.95745754]] [[0.8184377]]\n",
      "10075 [[0.09193264]] [[0.42322147]] [[0.7206311]]\n",
      "12071 [[0.49500018]] [[0.7533977]] [[0.84723455]]\n",
      "13905 [[0.2532935]] [[0.67281324]] [[0.5743401]]\n"
     ]
    }
   ],
   "source": [
    "keys = [   39,   338,   903,  1847,  2288,  4331,  6888,  8206,  8838,  8949,\n",
    "        9277,  9300,  9445,  9476, 10075, 12071, 13905]\n",
    "\n",
    "for sample_idx in keys:\n",
    "    # assert sample_idx in test_idx\n",
    "    sample = datamodule.dataset[sample_idx]\n",
    "    struct_latent = representations['struct'][sample_idx]\n",
    "    morph_latent = representations['morph'][sample_idx]\n",
    "    joint_latent = representations['joint'][sample_idx]\n",
    "    \n",
    "    print(\n",
    "        sample_idx,\n",
    "        cosine_similarity(struct_latent.reshape(1, -1), morph_latent.reshape(1, -1)),\n",
    "        cosine_similarity(struct_latent.reshape(1, -1), joint_latent.reshape(1, -1)),\n",
    "        cosine_similarity(morph_latent.reshape(1, -1), joint_latent.reshape(1, -1))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
